{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16e39cc6-55f7-4e7f-a213-88685a8befae",
   "metadata": {},
   "source": [
    "# Hypothesis Testing and Inference\n",
    "We often want to test whether a certain hypothesis is likely to be true. Hypotheses are typically assertions like \"this coin is fair' or \"data scienctist love python\" that can be translated into statistics about data. As we estabtilished before we can approximate a binomial distribution with a normal apporaximation such that\n",
    "\n",
    "$$ \\mu =  np \\quad\n",
    "\\sigma = \\sqrt{np(1-p)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5aee664-59ca-46e0-8c24-b1e0d0ce054e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200.0, 12.649110640673518)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "import math\n",
    "\n",
    "def normal_approximatiom_to_binomial(n: int, p: float) -> Tuple[float, float]:\n",
    "    \"\"\" Returns the mean and standard deviation corresponding to a Binomial(n,p) \"\"\"\n",
    "    mu = n*p\n",
    "    sigma = math.sqrt(n*p*(1-p))\n",
    "\n",
    "    return mu, sigma\n",
    "\n",
    "normal_approximatiom_to_binomial(1000,0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d87a671-0181-4e59-8b8c-3094a4b1206c",
   "metadata": {},
   "source": [
    "When a random variable follows a normal distribution, we can use normal_cdf to figure out the probability that its realised value lies within or outside a particular interval. As default normal_cdf() returns the area under the curve from a point and below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6909cf55-9010-4226-94b9-004ca469fdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Probabtility below Z=1: 0.8413447460685429\n",
      "The Probabtility above Z=1: 0.15865525393145707\n",
      "The Probabtility inbetween z=[-1,1]: 0.6826894921370859\n",
      "The Probabtility outside z=[-1,1]: 0.31731050786291415\n"
     ]
    }
   ],
   "source": [
    "from helpers import normal_cdf\n",
    "\n",
    "normal_probability_below = normal_cdf\n",
    "print(f\"The Probabtility below Z=1: {normal_probability_below(1)}\")\n",
    "\n",
    "#The probability that is is above the threshold\n",
    "def normal_probability_above(lo: float, mu: float=0, sigma: float=1) -> float:\n",
    "    \"\"\" Returns Probability = P(Z <= z) \"\"\"\n",
    "    return 1 - normal_cdf(lo,mu,sigma)\n",
    "\n",
    "print(f\"The Probabtility above Z=1: {normal_probability_above(1)}\")\n",
    "\n",
    "def normal_probability_between(lo: float, hi: float, mu: float=0, sigma: float=1) -> float:\n",
    "    return normal_cdf(hi, mu,sigma) - normal_cdf(lo, mu, sigma)\n",
    "\n",
    "print(f\"The Probabtility inbetween z=[-1,1]: {normal_probability_between(-1,1)}\")\n",
    "\n",
    "def normal_probability_outside(lo: float, hi: float, mu: float=0, sigma: float=1) -> float:\n",
    "    return 1-normal_probability_between(lo,hi,mu,sigma)\n",
    "\n",
    "normal_probability_outside(-1,1)\n",
    "print(f\"The Probabtility outside z=[-1,1]: {normal_probability_outside(-1,1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec34fc2a-a781-41bf-8135-d46ffadac3d2",
   "metadata": {},
   "source": [
    "We can do the reverse again. Say we are given a certain level of likelihood and we want to know what region thaat pertains to. For example, if we want to find the interval centered at the mean and containing 60% probability, then we find the cutoffs where the upper and lower tails each contain 20% of the probability (given symmmetry).\n",
    "\n",
    "\n",
    "Show the three intervals with a chart showing the area and bounds we want to compute.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60f5a1ea-022c-4f9c-849b-d57cf67e0692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The z, for which P(Z <= z) = 0.8413447460685429 is z = 0.9999847412109375\n",
      "The z, for which P(Z >= z) = 0.8413447460685429 is z = -0.9999847412109375\n",
      "The z, for which P(z_1 <= Z <= z_2) = 0.8413447460685429 is z = (-1.4096832275390625, 1.4096832275390625)\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from helpers import inverse_normal_cdf\n",
    "\n",
    "def normal_upper_bound(probability: float, mu: float=0, sigma: float=1) -> float:\n",
    "    \"\"\"Returns the z for which P(Z <= z) = probabilty \"\"\"\n",
    "    return inverse_normal_cdf(probability,mu, sigma)\n",
    "\n",
    "probability= 0.8413447460685429\n",
    "print(f\"The z, for which P(Z <= z) = {probability} is z = {normal_upper_bound(probability)}\") #we should get 1 looking at the results from above where The Probabtility below Z=1: 0.8413447460685429\n",
    "\n",
    "def normal_lower_bound(probability: float, mu: float=0, sigma: float=1) -> float:\n",
    "    \"\"\" Returns the z for which P(Z >= z) = probability \"\"\"\n",
    "    return inverse_normal_cdf(1-probability, mu, sigma)\n",
    "\n",
    "print(f\"The z, for which P(Z >= z) = {probability} is z = {normal_lower_bound(probability)}\") #we should get -1 looking at the results from above where The Probabtility below Z=1: 0.8413447460685429\n",
    "\n",
    "\n",
    "def normal_two_sided_bounds(probability: float, mu: float=0, sigma:float=1) -> Tuple[float, float]:\n",
    "    \"\"\" Returns the symmetric (about the mean) bounds that contain the specified prob \"\"\"\n",
    "    tail_probability = (1-probability)/2\n",
    "    upper_bound = normal_lower_bound(tail_probability, mu, sigma)\n",
    "    lower_bound = normal_upper_bound(tail_probability, mu, sigma)\n",
    "\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "print(f\"The z, for which P(z_1 <= Z <= z_2) = {probability} is z = {normal_two_sided_bounds(probability)}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82c631-b703-4662-8fb8-e2041d0b94da",
   "metadata": {},
   "source": [
    "## Example of Hypothesis test: is a coin fair?\n",
    "Say that we will flip a coin $n=1000$ times. If our hypothesis of fairness is true (that the coin is fair and therefore gives heads 50% of the time and tails 50% of the time), X shoulder be distributed approximately normally with mean $\\mu = 500 (=np=1000*0.5)$ and standard deviation, $\\sigma = 15.8 (=\\sqrt{np(1-p)}=\\sqrt{250})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edf224df-8dd6-4e55-83e7-108ee9527366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean = 500.0 , sigma = 15.811388300841896\n"
     ]
    }
   ],
   "source": [
    "num_of_flips = 1000\n",
    "probability = 0.5\n",
    "mu_0, sigma_0 = normal_approximatiom_to_binomial(num_of_flips, probability)\n",
    "print(f\"mean = {mu_0} , sigma = {sigma_0}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16403fb8-23f9-42ad-add5-a2d106f4349c",
   "metadata": {},
   "source": [
    "But the question remains, how willing are we to make a type 1 error (\"false positive\"), in which we reject the hypothesis even though it is true. For example, lets say we run the simulation 1000 times and we get a head 350 times, how can we know if we should take this to mean the coin is unfair?\n",
    "\n",
    "To do this, we must make a decision about significance. Lets define $H_0$ as the null hypothesis, that represents some default position and some alternative hypothesis $H_1$. We typically set the significance to 5% or 1% and say that we will reject the null hypothesis $H_0$ if it falls in outside a set of bounds. If we choose 5%, this is to say that our confidence interval is $95% (=100%-5%)$. Therefore, if the experiment produces values withinn this range we accept $H_0$, otherwise reject.\n",
    "\n",
    "Assume probability $p$ really equals 0.5, there is a 5% chance we observe an X that lies outside this interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "193899d9-0d8f-433b-b362-1f7908f74298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.0% Confidence interval (469.011020350622, 530.988979649378)\n"
     ]
    }
   ],
   "source": [
    "#given significance at 5%, we can compute the bounds that give us 95% probability - which is the confidence interval.\n",
    "prob=0.95\n",
    "lower, upper = normal_two_sided_bounds(prob, mu_0, sigma_0)\n",
    "print(f\"{prob*100}% Confidence interval ({lower}, {upper})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1439bf-65dc-4c05-80f1-c848473cbd1d",
   "metadata": {},
   "source": [
    "Imagine we get 530 heads after running the experiment. We can compute the p-values, which is the probability - assuming $H_0$ is true, that we would see a value at least as extreme as the one we actually observed. \n",
    "\n",
    "## 1. What a p-value is really doing\n",
    "\n",
    "- Imagine the **null hypothesis** $(H_0)$ is true.  \n",
    "- You collect data and calculate a test statistic (like the sample mean or number of successes).  \n",
    "- The **p-value** is the probability of seeing a statistic *at least as extreme* as yours, **if $(H_0)$ were actually true**.  \n",
    "\n",
    "So it‚Äôs not ‚Äúthe probability that the null is true.‚Äù  \n",
    "It‚Äôs ‚Äúhow surprising my data would be under the null.‚Äù\n",
    "\n",
    "## 2. Why p-value > significance means we don't reject \\(H_0\\)\n",
    "\n",
    "- The **significance level** $(\\alpha)$, often 0.05) is the cutoff for how much false-positive risk we‚Äôre willing to tolerate.  \n",
    "- If **p-value < $(\\alpha)$**:  \n",
    "  - The data is so extreme that it would rarely occur if $(H_0)$ were true.  \n",
    "  - ‚áí We reject $(H_0)$.  \n",
    "- If **p-value > $(\\alpha)$**:  \n",
    "  - The data is not unusually extreme under the null.  \n",
    "  - ‚áí We don‚Äôt have enough evidence to reject $(H_0)$.  \n",
    "\n",
    "Important: ‚ÄúDon‚Äôt reject‚Äù does **not** mean ‚Äúprove the null is true.‚Äù  \n",
    "It just means the evidence is insufficient to overturn it.\n",
    "\n",
    "\n",
    "## 3. Why multiply by 2 in a two-sided test\n",
    "\n",
    "- In a **one-sided test**, ‚Äúextreme‚Äù means large deviations in one direction only (e.g. ‚Äútoo many heads‚Äù).  \n",
    "- In a **two-sided test**, ‚Äúextreme‚Äù means large deviations in *either* direction (too many **or** too few).  \n",
    "- Therefore, the two-tailed p-value is **twice the one-tailed probability**:  \n",
    "  $$\n",
    "  p\\_\\text{two-sided} = 2 \\times P(\\text{statistic at least as extreme in one tail})\n",
    "  $$\n",
    "\n",
    "\n",
    "We use the Continuity correction of $z \\pm 0.5$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3246480b-bf75-490f-b145-2dfe68253ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many times did we get an extreme value: 68, therefore p-value= 0.068\n"
     ]
    }
   ],
   "source": [
    "def two_sided_p_value(x: float, mu: float=0, sigma: float=1) -> float:\n",
    "    \"\"\" How likely are we to see a value at least as extreme as x \"\"\"\n",
    "    if x >= mu:\n",
    "        return 2 * normal_probability_above(x, mu, sigma)\n",
    "    else:\n",
    "        return 2 * normal_probability_below(x, mu, sigma)\n",
    "\n",
    "two_sided_p_value(529.5, mu_0, sigma_0)\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "extreme_value_count = 0\n",
    "num_of_experiments = 1000\n",
    "for _ in range(num_of_experiments):\n",
    "    num_heads = sum(1 if random.random() < 0.5 else 0 for _ in range (1000)) \n",
    "    if num_heads >=530 or num_heads <= 470:\n",
    "        extreme_value_count +=1 \n",
    "\n",
    "print(f\"How many times did we get an extreme value: {extreme_value_count}, therefore p-value= { extreme_value_count/num_of_experiments}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18991e30-fba8-43fc-ad14-83b33a0df2f8",
   "metadata": {},
   "source": [
    "When we get a $p-value$ > 5% we the result we've gotten isn't unusally extreme under the null. Therefore we do not have enough evidence to ject $H_0$.\n",
    "\n",
    "To do good science, you should determine your hypotheses before looking at the data, you should clean your data without the hypothese in mind, and you should keep in mind that p-values are not substitutes for common sense.\n",
    "\n",
    "## Example: Running an A/B Test:\n",
    "Assume we are in charge of website experience optimization. One of our advertisers has developed a new energy drink targeted at data scientists, and thbe VP of Ads wants my help choosing between Adverisemnet A (\"tastes great!\" and ad B (\"less bias!\").\n",
    "\n",
    "We decide to run an experiement, randomly showing visitors one of the ads and tracking how many people click on each one. Lets say that 990 out of 1,000 A-viewers click their ad, while only 10 out of 1,000 B-viewers click their ad, you can confidently say A is the better ad.\n",
    "\n",
    "However, what if the difference isn't so large. Here we use $\\textit{statistical inference}$\n",
    "\n",
    "Lets say that $N_A$ people see ad A, $n_A$ of them click on it. We can think of each ad view as a Bernoulli trial (1 for clicked, 0 for not clicked) with $p_A$ probability that someone clicks. We want to observe the click rate $$\\hat{p}_A = n_A/N_A$$. \n",
    "\n",
    "For Ad A:\n",
    "- Let $X_{A1},...,X_{AN_A}$ is a Bernoulli(p_A)\n",
    "- The count of the clicks is $S_A = \\sum X_{Ai}$ is Binomial(N_A, p_A)\n",
    "\n",
    "From Central Limit Theory, if $N_A$ is large then the we know that $\\hat{p}_A$ is a normal random varialbe mean $p_A$ and standard deviation $\\sigma = \\sqrt{p_A(1-p_A)/N_A}$. So we know that:\n",
    "\n",
    "$$E[S_A] = N_Ap_A \\quad\n",
    "VAR[S_A] = N_Ap_A(1-p_A)\n",
    "$$\n",
    "\n",
    "Therefore to use these above values we can compute the mean and variance for the normal random variable, click rate:\n",
    "$$ \n",
    "E[\\hat{p_A}] = E[S_A/N_A] = \\frac{1}{N_A} E[S_A] = \\frac{1}{N_A} N_A p_A = p_A\n",
    "$$\n",
    "\n",
    "$$ \n",
    "VAR[\\hat{p_A}] = \\frac{1}{N_A^2}VAR[S_A] = \\frac{1}{N_A^2}N_Ap_A(1-p_A) = \\frac{1}{N_A}p_A(1-p_A)\n",
    "$$\n",
    "\n",
    "We want to test the null hypothesis:\n",
    "$$H_0: p_A = p_B $$\n",
    "in otherwords, saying that the click-through rate for each ad is the same. The probability is identical. The alternative hypothesis, \n",
    "$$H_1: p_A \\neq p_B $$\n",
    "\n",
    "So the paramter of interest is the difference between the two observed sample click-through rates, $\\hat{p}_A$, $\\hat{p}_B$\n",
    "$$\n",
    "\\hat{p}_B - \\hat{p}_A\n",
    "$$\n",
    "and this is taken to be normally distributed given that indiviually they are independent and normally distbributed with mean:\n",
    "\n",
    "$$\n",
    "E[\\hat{p}_B - \\hat{p}_A] = \\frac{1}{N_B} E[S_{B}] - \\frac{1}{N_A} E[S_{A}] = p_B - p_A\n",
    "$$\n",
    "\n",
    "$$\n",
    "VAR[\\hat{p}_B - \\hat{p}_A] = \\sigma_B^2 + \\sigma_A^2,\n",
    "$$\n",
    "using the identity $VAR(X-Y) = VAR(X) + VAR(-Y) + 2COV(X,-Y)$, and $VAR(-Y) = VAR(Y)$ and $COV(X,-Y)=0$\n",
    "\n",
    "### Constructing the Z-statistic\n",
    "\n",
    "This is essentially centering and scaling the normal random variable. The general recipe is:\n",
    "\n",
    "$$ ùëç = \\frac{observed ‚àí expected}{ standard¬†deviation under¬†ùêª_0}$$\n",
    "\n",
    "\n",
    "- Here, ‚Äúobserved‚Äù = $\\hat{p}_B - \\hat{p}_A$\n",
    "- ‚ÄúExpected under $ùêª_0 = 0$ (since if $p_A=p_B=p$), difference is 0) so the mean is zero. Under the null hypothesis we assume both groups share the same true rate, and since we don't know $p$ - the probability - we use the pooled estimate, $\\hat{p}$:\n",
    "$$ \\hat{p} = \\frac{N_A+N_B}{n_A+n_B} $$\n",
    "- ‚ÄúStandard deviation under $H_0= \\sqrt{\\hat{p}(1-\\hat{p})(\\frac{1}{N_B}+\\frac{1}{N_A}})$\n",
    "So:\n",
    "$$\n",
    "Z = \\frac{\\hat{p}_B - \\hat{p}_A}{\\sqrt{\\hat{p} (1-\\hat{p})(\\frac{1}{N_B}+\\frac{1}{N_A}})}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "1. Convert Z to p-value using standard normal.\n",
    "\n",
    "2. Compare to $\\alpha$ (0.05).\n",
    "\n",
    "3. Report both p-value and effect size (difference in proportions).\n",
    "\n",
    "p-value = probability of data ‚Äúas or more extreme‚Äù under $H_0$\n",
    "\n",
    "- If $p < \\alpha$ ‚Üí reject $H_0$\n",
    "\n",
    "- If $p > \\alpha$ ‚Üí insufficient evidence, do not reject.\n",
    "\n",
    "One-tailed = directional; two-tailed = non-directional.\n",
    "\n",
    "What the p-value actually measures\n",
    "\n",
    "- It‚Äôs not ‚Äúthe probability of being 1.14 SDs away‚Äù exactly.\n",
    "\n",
    "- It‚Äôs ‚Äúthe probability of seeing a difference at least this extreme (‚â§ -0.02 or ‚â• +0.02) if the true difference were zero.‚Äù\n",
    "\n",
    "That‚Äôs why we double the tail.\n",
    "\n",
    "### When to use one-tailed vs two-tailed\n",
    "\n",
    "**One-tailed**: use when your hypothesis is directional.\n",
    "Example: ‚ÄúWe believe Ad B has a higher click-through than Ad A.‚Äù\n",
    "Then you only care if \n",
    "\n",
    "$$p_B > p_A$$\n",
    "\n",
    "- The rejection region is one side of the normal curve.\n",
    "\n",
    "**Two-tailed**: use when your hypothesis is non-directional.\n",
    "Example: ‚ÄúWe believe Ad B is different from Ad A.‚Äù\n",
    "Then you must consider both \n",
    "\n",
    "$$p_B > p_A \\quad and \\quad p_B < p_A $$\n",
    "\n",
    "- So you split Œ± into two tails, and double the tail probability in the p-value.\n",
    "\n",
    "Rule of thumb: unless you had a very strong reason before the experiment to only care about one direction, you do a two-tailed test.\n",
    "\n",
    "### Confidence Intervals\n",
    "\n",
    "A 95% confidence interval for the difference is:\n",
    "\n",
    "$$\n",
    "(\\hat p_B-\\hat p_A) \\pm 1.96 \\times SE.\n",
    "$$\n",
    "\n",
    "If 0 lies outside this interval, it corresponds to rejecting \\(H_0\\) at the 5% level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ef010af-a765-48de-b3b0-ade352b69a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003189699706216853"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "#in code this looks like\n",
    "def estimate_parameters(N: int, n:int) -> Tuple[float,float]:\n",
    "    \"\"\" Returns the mean and standard deviation \"\"\"\n",
    "    p = n/N #the click through rate\n",
    "    sigma = math.sqrt(p*(1-p)/N)\n",
    "    return p, sigma\n",
    "\n",
    "estimate_parameters(1000,200)\n",
    "\n",
    "def a_b_test_statistic(N_A: int, n_A: int, N_B: int, n_B: int) -> float:\n",
    "    p_A, sigma_A = estimate_parameters(N_A, n_A)\n",
    "    p_B, sigma_B = estimate_parameters(N_B, n_B)\n",
    "    return (p_B - p_A)/ math.sqrt(sigma_A**2 + sigma_B **2)\n",
    "\n",
    "z = a_b_test_statistic(1000,200,1000,180)\n",
    "\n",
    "two_sided_p_value(z)\n",
    "\n",
    "z_1 = a_b_test_statistic(1000,200,1000,150)\n",
    "two_sided_p_value(z_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b871ba0-4db5-4707-9edc-891027fd62a8",
   "metadata": {},
   "source": [
    "# What does this all mean?\n",
    "\n",
    "## üìä How to frame it for a business audience\n",
    "\n",
    "### Example 1 (200 vs 180 clicks):\n",
    "\n",
    "$\\hat{P}_A = 20%$, $\\hat{P}_B = 18%$.\n",
    "\n",
    "Difference = 2 percentage points.\n",
    "\n",
    "$Z = -1.14$, $p-value = 0.25$ ‚Üí not statistically significant at 5%.\n",
    "\n",
    "Interpretation: With this sample size, the observed 2% gap could easily be noise. We‚Äôd need more data before concluding Ad A really outperforms Ad B.\n",
    "\n",
    "### Example 2 (200 vs 150 clicks):\n",
    "\n",
    "$\\hat{P}_A = 20%$, $\\hat{P}_B$ = 15%.\n",
    "\n",
    "Difference = 5 percentage points.\n",
    "\n",
    "$Z = -2.94$, $p-value = 0.003$ ‚Üí significant at 5%.\n",
    "\n",
    "Interpretation: The evidence strongly suggests Ad A is better. At scale, that 5% lift in CTR could mean thousands of extra conversions or significant added revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8fc3a4-0e3e-40b5-b2f9-6031875e702b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsfs]",
   "language": "python",
   "name": "conda-env-dsfs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
